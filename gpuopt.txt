benztimm@perlmutter:login26:/pscratch/sd/b/benztimm/csc746hw5/build> sh gpu-script.sh 
Running with 32 threads per block and 1 thread blocks
 Read data from the file ../data/zebra-gray-int8-4x 
==PROF== Connected to process 445821 (/pscratch/sd/b/benztimm/csc746hw5/build/sobel_gpu)
 GPU configuration: 1 blocks, 32 threads per block 
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 0: 0%....50%....100% - 13 passes
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 445821
[445821] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-06 00:20:26, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.01
    gpu__time_duration.avg                                                          second                           1.33
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           0.23
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.21
    SM Frequency                                                             cycle/usecond                         764.99
    Elapsed Cycles                                                                   cycle                  1,019,872,715
    Memory [%]                                                                           %                           0.03
    DRAM Throughput                                                                      %                           0.01
    Duration                                                                        second                           1.33
    L1/TEX Cache Throughput                                                              %                           3.16
    L2 Cache Throughput                                                                  %                           0.02
    SM Active Cycles                                                                 cycle                   9,443,349.79
    Compute (SM) [%]                                                                     %                           0.04
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full
          waves across all SMs. Look at Launch Statistics for more details.

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             32
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                             32
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel
          concurrently with other workloads, consider reducing the block size to have at least one block per
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)
          description for more details on launch configurations.

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             64
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           1.56
    Achieved Active Warps Per SM                                                      warp                           1.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference
          between calculated theoretical (50.0%) and measured achieved occupancy (1.6%) can be the result of warp
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on
          optimizing occupancy.

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                      5,730,671
    Branch Efficiency                                                                    %                          99.77
    Avg. Divergent Branches                                                                                         17.96
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 6851808 excessive sectors (9% of the
          total 73160558 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source
          locations. The CUDA Programming Guide
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional
          information on reducing uncoalesced device memory accesses.

Running with 32 threads per block and 4 thread blocks
 Read data from the file ../data/zebra-gray-int8-4x 
==PROF== Connected to process 455835 (/pscratch/sd/b/benztimm/csc746hw5/build/sobel_gpu)
 GPU configuration: 4 blocks, 32 threads per block 
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 0: 0%....50%....100% - 13 passes
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 455835
[455835] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-06 00:21:41, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.05
    gpu__time_duration.avg                                                         msecond                         399.89
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           0.93
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.22
    SM Frequency                                                             cycle/usecond                         765.01
    Elapsed Cycles                                                                   cycle                    305,920,058
    Memory [%]                                                                           %                           0.11
    DRAM Throughput                                                                      %                           0.05
    Duration                                                                       msecond                         399.89
    L1/TEX Cache Throughput                                                              %                           2.93
    L2 Cache Throughput                                                                  %                           0.09
    SM Active Cycles                                                                 cycle                  11,327,671.34
    Compute (SM) [%]                                                                     %                           0.12
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full
          waves across all SMs. Look at Launch Statistics for more details.

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           4
    Registers Per Thread                                                   register/thread                             32
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                            128
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 108
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel
          concurrently with other workloads, consider reducing the block size to have at least one block per
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)
          description for more details on launch configurations.

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             64
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           1.56
    Achieved Active Warps Per SM                                                      warp                           1.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference
          between calculated theoretical (50.0%) and measured achieved occupancy (1.6%) can be the result of warp
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on
          optimizing occupancy.

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                      5,730,677
    Branch Efficiency                                                                    %                          99.77
    Avg. Divergent Branches                                                                                         17.96
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 6851808 excessive sectors (9% of the
          total 73160558 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source
          locations. The CUDA Programming Guide
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional
          information on reducing uncoalesced device memory accesses.

Running with 32 threads per block and 16 thread blocks
 Read data from the file ../data/zebra-gray-int8-4x 
==PROF== Connected to process 457652 (/pscratch/sd/b/benztimm/csc746hw5/build/sobel_gpu)
 GPU configuration: 16 blocks, 32 threads per block 
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 0: 0%....50%....100% - 13 passes
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 457652
[457652] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-06 00:22:02, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.17
    gpu__time_duration.avg                                                         msecond                         104.62
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           3.70
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.21
    SM Frequency                                                             cycle/usecond                         764.94
    Elapsed Cycles                                                                   cycle                     80,029,625
    Memory [%]                                                                           %                           0.38
    DRAM Throughput                                                                      %                           0.17
    Duration                                                                       msecond                         104.62
    L1/TEX Cache Throughput                                                              %                           2.55
    L2 Cache Throughput                                                                  %                           0.34
    SM Active Cycles                                                                 cycle                  11,847,294.93
    Compute (SM) [%]                                                                     %                           0.45
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full
          waves across all SMs. Look at Launch Statistics for more details.

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                          16
    Registers Per Thread                                                   register/thread                             32
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                            512
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 108
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel
          concurrently with other workloads, consider reducing the block size to have at least one block per
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)
          description for more details on launch configurations.

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             64
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           1.56
    Achieved Active Warps Per SM                                                      warp                           1.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory This kernel's
          theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM The difference
          between calculated theoretical (50.0%) and measured achieved occupancy (1.6%) can be the result of warp
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on
          optimizing occupancy.

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                      5,730,701
    Branch Efficiency                                                                    %                          99.77
    Avg. Divergent Branches                                                                                         17.96
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 6851808 excessive sectors (9% of the
          total 73160558 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source
          locations. The CUDA Programming Guide
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional
          information on reducing uncoalesced device memory accesses.

Running with 32 threads per block and 64 thread blocks
 Read data from the file ../data/zebra-gray-int8-4x 
==PROF== Connected to process 458036 (/pscratch/sd/b/benztimm/csc746hw5/build/sobel_gpu)
 GPU configuration: 64 blocks, 32 threads per block 
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 0: 0%....50%....100% - 13 passes
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 458036
[458036] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-06 00:22:09, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.69
    gpu__time_duration.avg                                                         msecond                          26.24
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                          14.79
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.21
    SM Frequency                                                             cycle/usecond                         764.94
    Elapsed Cycles                                                                   cycle                     20,069,562
    Memory [%]                                                                           %                           1.51
    DRAM Throughput                                                                      %                           0.69
    Duration                                                                       msecond                          26.24
    L1/TEX Cache Throughput                                                              %                           2.54
    L2 Cache Throughput                                                                  %                           1.39
    SM Active Cycles                                                                 cycle                  11,876,484.11
    Compute (SM) [%]                                                                     %                           1.81
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full
          waves across all SMs. Look at Launch Statistics for more details.

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                          64
    Registers Per Thread                                                   register/thread                             32
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          2,048
    Waves Per SM                                                                                                     0.02
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 108
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel
          concurrently with other workloads, consider reducing the block size to have at least one block per
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)
          description for more details on launch configurations.

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             64
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           1.56
    Achieved Active Warps Per SM                                                      warp                              1
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference
          between calculated theoretical (50.0%) and measured achieved occupancy (1.6%) can be the result of warp
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on
          optimizing occupancy.

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                      5,730,797
    Branch Efficiency                                                                    %                          99.77
    Avg. Divergent Branches                                                                                         17.96
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 6851808 excessive sectors (9% of the
          total 73160558 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source
          locations. The CUDA Programming Guide
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional
          information on reducing uncoalesced device memory accesses.

Running with 32 threads per block and 256 thread blocks
 Read data from the file ../data/zebra-gray-int8-4x 
==PROF== Connected to process 458507 (/pscratch/sd/b/benztimm/csc746hw5/build/sobel_gpu)
 GPU configuration: 256 blocks, 32 threads per block 
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 0: 0%....50%....100% - 13 passes
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 458507
[458507] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-06 00:22:14, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           2.55
    gpu__time_duration.avg                                                         msecond                           7.12
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                          59.21
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.21
    SM Frequency                                                             cycle/usecond                         763.70
    Elapsed Cycles                                                                   cycle                      5,433,761
    Memory [%]                                                                           %                           5.60
    DRAM Throughput                                                                      %                           2.55
    Duration                                                                       msecond                           7.12
    L1/TEX Cache Throughput                                                              %                           5.60
    L2 Cache Throughput                                                                  %                           4.65
    SM Active Cycles                                                                 cycle                   5,430,471.89
    Compute (SM) [%]                                                                     %                           6.67
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full
          waves across all SMs. Look at Launch Statistics for more details.

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         256
    Registers Per Thread                                                   register/thread                             32
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          8,192
    Waves Per SM                                                                                                     0.07
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             64
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           3.70
    Achieved Active Warps Per SM                                                      warp                           2.37
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference
          between calculated theoretical (50.0%) and measured achieved occupancy (3.7%) can be the result of warp
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on
          optimizing occupancy.

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                      5,731,181
    Branch Efficiency                                                                    %                          99.77
    Avg. Divergent Branches                                                                                         17.96
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 6851808 excessive sectors (9% of the
          total 73160558 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source
          locations. The CUDA Programming Guide
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional
          information on reducing uncoalesced device memory accesses.

Running with 32 threads per block and 1024 thread blocks
 Read data from the file ../data/zebra-gray-int8-4x 
==PROF== Connected to process 458556 (/pscratch/sd/b/benztimm/csc746hw5/build/sobel_gpu)
 GPU configuration: 1024 blocks, 32 threads per block 
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 0: 0%....50%....100% - 13 passes
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 458556
[458556] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-06 00:22:17, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           8.29
    gpu__time_duration.avg                                                         msecond                           2.19
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                          99.19
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.21
    SM Frequency                                                             cycle/usecond                         762.39
    Elapsed Cycles                                                                   cycle                      1,671,466
    Memory [%]                                                                           %                          18.54
    DRAM Throughput                                                                      %                           8.29
    Duration                                                                       msecond                           2.19
    L1/TEX Cache Throughput                                                              %                          18.64
    L2 Cache Throughput                                                                  %                          16.06
    SM Active Cycles                                                                 cycle                   1,662,709.07
    Compute (SM) [%]                                                                     %                          21.70
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full
          waves across all SMs. Look at Launch Statistics for more details.

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                       1,024
    Registers Per Thread                                                   register/thread                             32
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                         32,768
    Waves Per SM                                                                                                     0.30
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             64
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                          14.75
    Achieved Active Warps Per SM                                                      warp                           9.44
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference
          between calculated theoretical (50.0%) and measured achieved occupancy (14.8%) can be the result of warp
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on
          optimizing occupancy.

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                      5,732,717
    Branch Efficiency                                                                    %                          99.77
    Avg. Divergent Branches                                                                                         17.96
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 6851808 excessive sectors (9% of the
          total 73160558 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source
          locations. The CUDA Programming Guide
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional
          information on reducing uncoalesced device memory accesses.

Running with 32 threads per block and 4096 thread blocks
 Read data from the file ../data/zebra-gray-int8-4x 
==PROF== Connected to process 458596 (/pscratch/sd/b/benztimm/csc746hw5/build/sobel_gpu)
 GPU configuration: 4096 blocks, 32 threads per block 
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 0: 0%....50%....100% - 13 passes
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 458596
[458596] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-06 00:22:20, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                          16.34
    gpu__time_duration.avg                                                         msecond                           1.24
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                          98.32
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.22
    SM Frequency                                                             cycle/usecond                         765.28
    Elapsed Cycles                                                                   cycle                        952,431
    Memory [%]                                                                           %                          32.58
    DRAM Throughput                                                                      %                          16.34
    Duration                                                                       msecond                           1.24
    L1/TEX Cache Throughput                                                              %                          32.90
    L2 Cache Throughput                                                                  %                          29.37
    SM Active Cycles                                                                 cycle                     943,051.67
    Compute (SM) [%]                                                                     %                          38.10
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                       4,096
    Registers Per Thread                                                   register/thread                             32
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                        131,072
    Waves Per SM                                                                                                     1.19
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 639 thread blocks.
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for
          up to 50.0% of the total kernel runtime with a lower occupancy of 34.1%. Try launching a grid with no
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for
          a grid. See the Hardware Model
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more
          details on launch configurations.

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             64
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                          32.97
    Achieved Active Warps Per SM                                                      warp                          21.10
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory This kernel's
          theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM The difference
          between calculated theoretical (50.0%) and measured achieved occupancy (33.0%) can be the result of warp
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on
          optimizing occupancy.

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                      5,738,861
    Branch Efficiency                                                                    %                          99.77
    Avg. Divergent Branches                                                                                         17.96
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 6851808 excessive sectors (9% of the
          total 73160558 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source
          locations. The CUDA Programming Guide
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional
          information on reducing uncoalesced device memory accesses.

Running with 64 threads per block and 1 thread blocks
 Read data from the file ../data/zebra-gray-int8-4x 
==PROF== Connected to process 458838 (/pscratch/sd/b/benztimm/csc746hw5/build/sobel_gpu)
 GPU configuration: 1 blocks, 64 threads per block 
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 0: 0%....50%....100% - 13 passes
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 458838
[458838] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-06 00:24:45, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.03
    gpu__time_duration.avg                                                         msecond                         673.58
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           0.46
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.21
    SM Frequency                                                             cycle/usecond                         764.98
    Elapsed Cycles                                                                   cycle                    515,275,174
    Memory [%]                                                                           %                           0.06
    DRAM Throughput                                                                      %                           0.03
    Duration                                                                       msecond                         673.58
    L1/TEX Cache Throughput                                                              %                           6.26
    L2 Cache Throughput                                                                  %                           0.04
    SM Active Cycles                                                                 cycle                   4,771,376.67
    Compute (SM) [%]                                                                     %                           0.07
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full
          waves across all SMs. Look at Launch Statistics for more details.

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         64
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             32
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                             64
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel
          concurrently with other workloads, consider reducing the block size to have at least one block per
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)
          description for more details on launch configurations.

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             32
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             32
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                           3.12
    Achieved Active Warps Per SM                                                      warp                           2.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated
          theoretical (100.0%) and measured achieved occupancy (3.1%) can be the result of warp scheduling overheads
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on
          optimizing occupancy.

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                      5,730,673
    Branch Efficiency                                                                    %                          99.77
    Avg. Divergent Branches                                                                                         17.96
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 6851808 excessive sectors (9% of the
          total 73160558 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source
          locations. The CUDA Programming Guide
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional
          information on reducing uncoalesced device memory accesses.

Running with 64 threads per block and 4 thread blocks
 Read data from the file ../data/zebra-gray-int8-4x 
==PROF== Connected to process 464537 (/pscratch/sd/b/benztimm/csc746hw5/build/sobel_gpu)
 GPU configuration: 4 blocks, 64 threads per block 
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 0: 0%....50%....100% - 13 passes
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 464537
[464537] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-06 00:25:24, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.09
    gpu__time_duration.avg                                                         msecond                         205.16
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           1.84
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.22
    SM Frequency                                                             cycle/usecond                         766.90
    Elapsed Cycles                                                                   cycle                    157,335,758
    Memory [%]                                                                           %                           0.21
    DRAM Throughput                                                                      %                           0.09
    Duration                                                                       msecond                         205.16
    L1/TEX Cache Throughput                                                              %                           5.61
    L2 Cache Throughput                                                                  %                           0.16
    SM Active Cycles                                                                 cycle                   5,878,472.57
    Compute (SM) [%]                                                                     %                           0.23
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full
          waves across all SMs. Look at Launch Statistics for more details.

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         64
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           4
    Registers Per Thread                                                   register/thread                             32
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                            256
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 108
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel
          concurrently with other workloads, consider reducing the block size to have at least one block per
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)
          description for more details on launch configurations.

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             32
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             32
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                           3.12
    Achieved Active Warps Per SM                                                      warp                           2.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated
          theoretical (100.0%) and measured achieved occupancy (3.1%) can be the result of warp scheduling overheads
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on
          optimizing occupancy.

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                      5,730,685
    Branch Efficiency                                                                    %                          99.77
    Avg. Divergent Branches                                                                                         17.96
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 6851808 excessive sectors (9% of the
          total 73160558 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source
          locations. The CUDA Programming Guide
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional
          information on reducing uncoalesced device memory accesses.

Running with 64 threads per block and 16 thread blocks
 Read data from the file ../data/zebra-gray-int8-4x 
==PROF== Connected to process 466371 (/pscratch/sd/b/benztimm/csc746hw5/build/sobel_gpu)
 GPU configuration: 16 blocks, 64 threads per block 
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 0: 0%....50%....100% - 13 passes
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 466371
[466371] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-06 00:25:36, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.33
    gpu__time_duration.avg                                                         msecond                          55.58
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           7.64
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.20
    SM Frequency                                                             cycle/usecond                         753.61
    Elapsed Cycles                                                                   cycle                     41,886,270
    Memory [%]                                                                           %                           0.81
    DRAM Throughput                                                                      %                           0.33
    Duration                                                                       msecond                          55.58
    L1/TEX Cache Throughput                                                              %                           5.37
    L2 Cache Throughput                                                                  %                           0.57
    SM Active Cycles                                                                 cycle                   6,316,104.16
    Compute (SM) [%]                                                                     %                           0.87
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full
          waves across all SMs. Look at Launch Statistics for more details.

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         64
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                          16
    Registers Per Thread                                                   register/thread                             32
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          1,024
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 108
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel
          concurrently with other workloads, consider reducing the block size to have at least one block per
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)
          description for more details on launch configurations.

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             32
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             32
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                           3.12
    Achieved Active Warps Per SM                                                      warp                           2.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated
          theoretical (100.0%) and measured achieved occupancy (3.1%) can be the result of warp scheduling overheads
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on
          optimizing occupancy.

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                      5,730,733
    Branch Efficiency                                                                    %                          99.77
    Avg. Divergent Branches                                                                                         17.96
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 6851808 excessive sectors (9% of the
          total 73160558 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source
          locations. The CUDA Programming Guide
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional
          information on reducing uncoalesced device memory accesses.

Running with 64 threads per block and 64 thread blocks
 Read data from the file ../data/zebra-gray-int8-4x 
==PROF== Connected to process 467292 (/pscratch/sd/b/benztimm/csc746hw5/build/sobel_gpu)
 GPU configuration: 64 blocks, 64 threads per block 
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 0: 0%....50%....100% - 13 passes
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 467292
[467292] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-06 00:25:42, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           1.35
    gpu__time_duration.avg                                                         msecond                          13.46
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                          29.67
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.21
    SM Frequency                                                             cycle/usecond                         763.44
    Elapsed Cycles                                                                   cycle                     10,273,494
    Memory [%]                                                                           %                           2.99
    DRAM Throughput                                                                      %                           1.35
    Duration                                                                       msecond                          13.46
    L1/TEX Cache Throughput                                                              %                           5.05
    L2 Cache Throughput                                                                  %                           2.63
    SM Active Cycles                                                                 cycle                   6,088,562.45
    Compute (SM) [%]                                                                     %                           3.53
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full
          waves across all SMs. Look at Launch Statistics for more details.

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         64
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                          64
    Registers Per Thread                                                   register/thread                             32
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          4,096
    Waves Per SM                                                                                                     0.02
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 108
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel
          concurrently with other workloads, consider reducing the block size to have at least one block per
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)
          description for more details on launch configurations.

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             32
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             32
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                           3.12
    Achieved Active Warps Per SM                                                      warp                           2.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated
          theoretical (100.0%) and measured achieved occupancy (3.1%) can be the result of warp scheduling overheads
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on
          optimizing occupancy.

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                      5,730,925
    Branch Efficiency                                                                    %                          99.77
    Avg. Divergent Branches                                                                                         17.96
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 6851808 excessive sectors (9% of the
          total 73160558 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source
          locations. The CUDA Programming Guide
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional
          information on reducing uncoalesced device memory accesses.

Running with 64 threads per block and 256 thread blocks
 Read data from the file ../data/zebra-gray-int8-4x 
==PROF== Connected to process 467340 (/pscratch/sd/b/benztimm/csc746hw5/build/sobel_gpu)
 GPU configuration: 256 blocks, 64 threads per block 
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 0: 0%....50%....100% - 13 passes
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 467340
[467340] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-06 00:25:45, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           4.49
    gpu__time_duration.avg                                                         msecond                           4.05
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                          91.48
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.21
    SM Frequency                                                             cycle/usecond                         760.39
    Elapsed Cycles                                                                   cycle                      3,079,492
    Memory [%]                                                                           %                          10.25
    DRAM Throughput                                                                      %                           4.49
    Duration                                                                       msecond                           4.05
    L1/TEX Cache Throughput                                                              %                          10.23
    L2 Cache Throughput                                                                  %                           8.36
    SM Active Cycles                                                                 cycle                   3,083,100.23
    Compute (SM) [%]                                                                     %                          11.78
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full
          waves across all SMs. Look at Launch Statistics for more details.

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         64
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         256
    Registers Per Thread                                                   register/thread                             32
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                         16,384
    Waves Per SM                                                                                                     0.07
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             32
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             32
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                           7.40
    Achieved Active Warps Per SM                                                      warp                           4.74
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated
          theoretical (100.0%) and measured achieved occupancy (7.4%) can be the result of warp scheduling overheads
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on
          optimizing occupancy.

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                      5,731,693
    Branch Efficiency                                                                    %                          99.77
    Avg. Divergent Branches                                                                                         17.96
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 6851808 excessive sectors (9% of the
          total 73160558 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source
          locations. The CUDA Programming Guide
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional
          information on reducing uncoalesced device memory accesses.

Running with 64 threads per block and 1024 thread blocks
 Read data from the file ../data/zebra-gray-int8-4x 
==PROF== Connected to process 467387 (/pscratch/sd/b/benztimm/csc746hw5/build/sobel_gpu)
 GPU configuration: 1024 blocks, 64 threads per block 
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 0: 0%....50%....100% - 13 passes
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 467387
[467387] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-06 00:25:49, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                          14.29
    gpu__time_duration.avg                                                         msecond                           1.27
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                          97.53
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.21
    SM Frequency                                                             cycle/usecond                         762.79
    Elapsed Cycles                                                                   cycle                        971,091
    Memory [%]                                                                           %                          33.38
    DRAM Throughput                                                                      %                          14.29
    Duration                                                                       msecond                           1.27
    L1/TEX Cache Throughput                                                              %                          33.99
    L2 Cache Throughput                                                                  %                          26.78
    SM Active Cycles                                                                 cycle                     953,492.85
    Compute (SM) [%]                                                                     %                          37.36
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full
          waves across all SMs. Look at Launch Statistics for more details.

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         64
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                       1,024
    Registers Per Thread                                                   register/thread                             32
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                         65,536
    Waves Per SM                                                                                                     0.30
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             32
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             32
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          28.95
    Achieved Active Warps Per SM                                                      warp                          18.53
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated
          theoretical (100.0%) and measured achieved occupancy (28.9%) can be the result of warp scheduling overheads
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on
          optimizing occupancy.

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                      5,734,765
    Branch Efficiency                                                                    %                          99.77
    Avg. Divergent Branches                                                                                         17.96
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 6851808 excessive sectors (9% of the
          total 73160558 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source
          locations. The CUDA Programming Guide
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional
          information on reducing uncoalesced device memory accesses.

Running with 64 threads per block and 4096 thread blocks
 Read data from the file ../data/zebra-gray-int8-4x 
==PROF== Connected to process 467606 (/pscratch/sd/b/benztimm/csc746hw5/build/sobel_gpu)
 GPU configuration: 4096 blocks, 64 threads per block 
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 0: 0%....50%....100% - 13 passes
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 467606
[467606] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-06 00:25:52, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                          25.60
    gpu__time_duration.avg                                                         usecond                         768.58
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                          97.34
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.22
    SM Frequency                                                             cycle/usecond                         765.42
    Elapsed Cycles                                                                   cycle                        588,294
    Memory [%]                                                                           %                          55.45
    DRAM Throughput                                                                      %                          25.60
    Duration                                                                       usecond                         768.58
    L1/TEX Cache Throughput                                                              %                          56.74
    L2 Cache Throughput                                                                  %                          44.19
    SM Active Cycles                                                                 cycle                     574,866.01
    Compute (SM) [%]                                                                     %                          61.71
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced.
          Check both the Compute Workload Analysis and Memory Workload Analysis sections.

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         64
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                       4,096
    Registers Per Thread                                                   register/thread                             32
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                        262,144
    Waves Per SM                                                                                                     1.19
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 639 thread blocks.
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for
          up to 50.0% of the total kernel runtime with a lower occupancy of 28.4%. Try launching a grid with no
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for
          a grid. See the Hardware Model
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more
          details on launch configurations.

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             32
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             32
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          71.60
    Achieved Active Warps Per SM                                                      warp                          45.82
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated
          theoretical (100.0%) and measured achieved occupancy (71.6%) can be the result of warp scheduling overheads
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on
          optimizing occupancy.

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                      5,747,053
    Branch Efficiency                                                                    %                          99.77
    Avg. Divergent Branches                                                                                         17.96
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 6851808 excessive sectors (9% of the
          total 73160558 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source
          locations. The CUDA Programming Guide
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional
          information on reducing uncoalesced device memory accesses.

Running with 128 threads per block and 1 thread blocks
 Read data from the file ../data/zebra-gray-int8-4x 
==PROF== Connected to process 467707 (/pscratch/sd/b/benztimm/csc746hw5/build/sobel_gpu)
 GPU configuration: 1 blocks, 128 threads per block 
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 0: 0%....50%....100% - 13 passes
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 467707
[467707] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-06 00:27:07, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.05
    gpu__time_duration.avg                                                         msecond                         360.55
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           0.92
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.21
    SM Frequency                                                             cycle/usecond                         762.46
    Elapsed Cycles                                                                   cycle                    274,902,338
    Memory [%]                                                                           %                           0.11
    DRAM Throughput                                                                      %                           0.05
    Duration                                                                       msecond                         360.55
    L1/TEX Cache Throughput                                                              %                          12.30
    L2 Cache Throughput                                                                  %                           0.07
    SM Active Cycles                                                                 cycle                   2,557,362.06
    Compute (SM) [%]                                                                     %                           0.13
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full
          waves across all SMs. Look at Launch Statistics for more details.

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             32
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                            128
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel
          concurrently with other workloads, consider reducing the block size to have at least one block per
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)
          description for more details on launch configurations.

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             16
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             16
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                           6.25
    Achieved Active Warps Per SM                                                      warp                           4.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated
          theoretical (100.0%) and measured achieved occupancy (6.2%) can be the result of warp scheduling overheads
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on
          optimizing occupancy.

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                      5,730,677
    Branch Efficiency                                                                    %                          99.77
    Avg. Divergent Branches                                                                                         17.96
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 6851808 excessive sectors (9% of the
          total 73160558 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source
          locations. The CUDA Programming Guide
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional
          information on reducing uncoalesced device memory accesses.

Running with 128 threads per block and 4 thread blocks
 Read data from the file ../data/zebra-gray-int8-4x 
==PROF== Connected to process 470065 (/pscratch/sd/b/benztimm/csc746hw5/build/sobel_gpu)
 GPU configuration: 4 blocks, 128 threads per block 
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 0: 0%....50%....100% - 13 passes
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 470065
[470065] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-06 00:27:28, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.15
    gpu__time_duration.avg                                                         msecond                         114.82
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           3.69
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.24
    SM Frequency                                                             cycle/usecond                         777.97
    Elapsed Cycles                                                                   cycle                     89,326,436
    Memory [%]                                                                           %                           0.38
    DRAM Throughput                                                                      %                           0.15
    Duration                                                                       msecond                         114.82
    L1/TEX Cache Throughput                                                              %                          10.34
    L2 Cache Throughput                                                                  %                           0.27
    SM Active Cycles                                                                 cycle                   3,286,006.25
    Compute (SM) [%]                                                                     %                           0.41
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full
          waves across all SMs. Look at Launch Statistics for more details.

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           4
    Registers Per Thread                                                   register/thread                             32
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                            512
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 108
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel
          concurrently with other workloads, consider reducing the block size to have at least one block per
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)
          description for more details on launch configurations.

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             16
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             16
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                           6.25
    Achieved Active Warps Per SM                                                      warp                           4.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated
          theoretical (100.0%) and measured achieved occupancy (6.2%) can be the result of warp scheduling overheads
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on
          optimizing occupancy.

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                      5,730,701
    Branch Efficiency                                                                    %                          99.77
    Avg. Divergent Branches                                                                                         17.96
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 6851808 excessive sectors (9% of the
          total 73160558 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source
          locations. The CUDA Programming Guide
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional
          information on reducing uncoalesced device memory accesses.

Running with 128 threads per block and 16 thread blocks
 Read data from the file ../data/zebra-gray-int8-4x 
==PROF== Connected to process 470418 (/pscratch/sd/b/benztimm/csc746hw5/build/sobel_gpu)
 GPU configuration: 16 blocks, 128 threads per block 
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 0: 0%....50%....100% - 13 passes
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 470418
[470418] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-06 00:27:35, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.65
    gpu__time_duration.avg                                                         msecond                          27.92
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                          14.81
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.21
    SM Frequency                                                             cycle/usecond                         763.23
    Elapsed Cycles                                                                   cycle                     21,312,561
    Memory [%]                                                                           %                           1.54
    DRAM Throughput                                                                      %                           0.65
    Duration                                                                       msecond                          27.92
    L1/TEX Cache Throughput                                                              %                          10.45
    L2 Cache Throughput                                                                  %                           1.05
    SM Active Cycles                                                                 cycle                   3,148,820.46
    Compute (SM) [%]                                                                     %                           1.70
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full
          waves across all SMs. Look at Launch Statistics for more details.

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                          16
    Registers Per Thread                                                   register/thread                             32
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          2,048
    Waves Per SM                                                                                                     0.01
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 108
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel
          concurrently with other workloads, consider reducing the block size to have at least one block per
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)
          description for more details on launch configurations.

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             16
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             16
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                           6.25
    Achieved Active Warps Per SM                                                      warp                           4.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated
          theoretical (100.0%) and measured achieved occupancy (6.2%) can be the result of warp scheduling overheads
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on
          optimizing occupancy.

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                      5,730,797
    Branch Efficiency                                                                    %                          99.77
    Avg. Divergent Branches                                                                                         17.96
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 6851808 excessive sectors (9% of the
          total 73160558 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source
          locations. The CUDA Programming Guide
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional
          information on reducing uncoalesced device memory accesses.

Running with 128 threads per block and 64 thread blocks
 Read data from the file ../data/zebra-gray-int8-4x 
==PROF== Connected to process 470683 (/pscratch/sd/b/benztimm/csc746hw5/build/sobel_gpu)
 GPU configuration: 64 blocks, 128 threads per block 
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 0: 0%....50%....100% - 13 passes
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 470683
[470683] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-06 00:27:40, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           2.48
    gpu__time_duration.avg                                                         msecond                           7.29
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                          58.50
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.22
    SM Frequency                                                             cycle/usecond                         765.33
    Elapsed Cycles                                                                   cycle                      5,582,040
    Memory [%]                                                                           %                           5.58
    DRAM Throughput                                                                      %                           2.48
    Duration                                                                       msecond                           7.29
    L1/TEX Cache Throughput                                                              %                           9.54
    L2 Cache Throughput                                                                  %                           4.57
    SM Active Cycles                                                                 cycle                   3,261,254.69
    Compute (SM) [%]                                                                     %                           6.50
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full
          waves across all SMs. Look at Launch Statistics for more details.

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                          64
    Registers Per Thread                                                   register/thread                             32
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          8,192
    Waves Per SM                                                                                                     0.04
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 108
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel
          concurrently with other workloads, consider reducing the block size to have at least one block per
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)
          description for more details on launch configurations.

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             16
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             16
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                           6.25
    Achieved Active Warps Per SM                                                      warp                           4.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated
          theoretical (100.0%) and measured achieved occupancy (6.2%) can be the result of warp scheduling overheads
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on
          optimizing occupancy.

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                      5,731,181
    Branch Efficiency                                                                    %                          99.77
    Avg. Divergent Branches                                                                                         17.96
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 6851808 excessive sectors (9% of the
          total 73160558 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source
          locations. The CUDA Programming Guide
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional
          information on reducing uncoalesced device memory accesses.

Running with 128 threads per block and 256 thread blocks
 Read data from the file ../data/zebra-gray-int8-4x 
==PROF== Connected to process 470756 (/pscratch/sd/b/benztimm/csc746hw5/build/sobel_gpu)
 GPU configuration: 256 blocks, 128 threads per block 
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 0: 0%....50%....100% - 13 passes
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 470756
[470756] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-06 00:27:43, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           8.28
    gpu__time_duration.avg                                                         msecond                           2.19
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                          99.04
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.21
    SM Frequency                                                             cycle/usecond                         764.25
    Elapsed Cycles                                                                   cycle                      1,671,373
    Memory [%]                                                                           %                          19.21
    DRAM Throughput                                                                      %                           8.28
    Duration                                                                       msecond                           2.19
    L1/TEX Cache Throughput                                                              %                          19.37
    L2 Cache Throughput                                                                  %                          14.97
    SM Active Cycles                                                                 cycle                   1,658,155.11
    Compute (SM) [%]                                                                     %                          21.70
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full
          waves across all SMs. Look at Launch Statistics for more details.

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         256
    Registers Per Thread                                                   register/thread                             32
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                         32,768
    Waves Per SM                                                                                                     0.15
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             16
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             16
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          14.77
    Achieved Active Warps Per SM                                                      warp                           9.46
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated
          theoretical (100.0%) and measured achieved occupancy (14.8%) can be the result of warp scheduling overheads
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on
          optimizing occupancy.

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                      5,732,717
    Branch Efficiency                                                                    %                          99.77
    Avg. Divergent Branches                                                                                         17.96
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 6851808 excessive sectors (9% of the
          total 73160558 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source
          locations. The CUDA Programming Guide
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional
          information on reducing uncoalesced device memory accesses.

Running with 128 threads per block and 1024 thread blocks
 Read data from the file ../data/zebra-gray-int8-4x 
==PROF== Connected to process 470803 (/pscratch/sd/b/benztimm/csc746hw5/build/sobel_gpu)
 GPU configuration: 1024 blocks, 128 threads per block 
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 0: 0%....50%....100% - 13 passes
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 470803
[470803] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-06 00:27:47, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                          22.72
    gpu__time_duration.avg                                                         usecond                         831.04
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                          97.19
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.21
    SM Frequency                                                             cycle/usecond                         763.42
    Elapsed Cycles                                                                   cycle                        634,441
    Memory [%]                                                                           %                          52.04
    DRAM Throughput                                                                      %                          22.72
    Duration                                                                       usecond                         831.04
    L1/TEX Cache Throughput                                                              %                          53.44
    L2 Cache Throughput                                                                  %                          37.56
    SM Active Cycles                                                                 cycle                     617,761.63
    Compute (SM) [%]                                                                     %                          57.19
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.6 full
          waves across all SMs. Look at Launch Statistics for more details.

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                       1,024
    Registers Per Thread                                                   register/thread                             32
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                        131,072
    Waves Per SM                                                                                                     0.59
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             16
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             16
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          54.17
    Achieved Active Warps Per SM                                                      warp                          34.67
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated
          theoretical (100.0%) and measured achieved occupancy (54.2%) can be the result of warp scheduling overheads
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on
          optimizing occupancy.

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                      5,738,861
    Branch Efficiency                                                                    %                          99.77
    Avg. Divergent Branches                                                                                         17.96
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 6851808 excessive sectors (9% of the
          total 73160558 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source
          locations. The CUDA Programming Guide
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional
          information on reducing uncoalesced device memory accesses.

Running with 128 threads per block and 4096 thread blocks
 Read data from the file ../data/zebra-gray-int8-4x 
==PROF== Connected to process 470861 (/pscratch/sd/b/benztimm/csc746hw5/build/sobel_gpu)
 GPU configuration: 4096 blocks, 128 threads per block 
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 0: 0%....50%....100% - 13 passes
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 470861
[470861] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-06 00:27:50, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                          28.17
    gpu__time_duration.avg                                                         usecond                         669.73
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                          98.15
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.21
    SM Frequency                                                             cycle/usecond                         761.10
    Elapsed Cycles                                                                   cycle                        509,739
    Memory [%]                                                                           %                          65.16
    DRAM Throughput                                                                      %                          28.17
    Duration                                                                       usecond                         669.73
    L1/TEX Cache Throughput                                                              %                          66.29
    L2 Cache Throughput                                                                  %                          48.25
    SM Active Cycles                                                                 cycle                     501,062.07
    Compute (SM) [%]                                                                     %                          71.28
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced.
          Check both the Compute Workload Analysis and Memory Workload Analysis sections.

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                       4,096
    Registers Per Thread                                                   register/thread                             32
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                        524,288
    Waves Per SM                                                                                                     2.37
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             16
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             16
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          85.52
    Achieved Active Warps Per SM                                                      warp                          54.73
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated
          theoretical (100.0%) and measured achieved occupancy (85.5%) can be the result of warp scheduling overheads
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on
          optimizing occupancy.

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                      5,763,437
    Branch Efficiency                                                                    %                          99.77
    Avg. Divergent Branches                                                                                         17.96
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 6851808 excessive sectors (9% of the
          total 73160558 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source
          locations. The CUDA Programming Guide
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional
          information on reducing uncoalesced device memory accesses.

Running with 256 threads per block and 1 thread blocks
 Read data from the file ../data/zebra-gray-int8-4x 
==PROF== Connected to process 471082 (/pscratch/sd/b/benztimm/csc746hw5/build/sobel_gpu)
 GPU configuration: 1 blocks, 256 threads per block 
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 0: 0%....50%....100% - 13 passes
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 471082
[471082] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-06 00:28:31, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.08
    gpu__time_duration.avg                                                         msecond                         232.45
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           0.94
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.18
    SM Frequency                                                             cycle/usecond                         744.84
    Elapsed Cycles                                                                   cycle                    173,137,972
    Memory [%]                                                                           %                           0.19
    DRAM Throughput                                                                      %                           0.08
    Duration                                                                       msecond                         232.45
    L1/TEX Cache Throughput                                                              %                          20.45
    L2 Cache Throughput                                                                  %                           0.11
    SM Active Cycles                                                                 cycle                   1,636,332.24
    Compute (SM) [%]                                                                     %                           0.21
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full
          waves across all SMs. Look at Launch Statistics for more details.

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             32
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                            256
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel
          concurrently with other workloads, consider reducing the block size to have at least one block per
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)
          description for more details on launch configurations.

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              8
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              8
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          12.50
    Achieved Active Warps Per SM                                                      warp                           8.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated
          theoretical (100.0%) and measured achieved occupancy (12.5%) can be the result of warp scheduling overheads
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on
          optimizing occupancy.

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                      5,730,685
    Branch Efficiency                                                                    %                          99.77
    Avg. Divergent Branches                                                                                         17.96
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 6851808 excessive sectors (9% of the
          total 73160558 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source
          locations. The CUDA Programming Guide
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional
          information on reducing uncoalesced device memory accesses.

Running with 256 threads per block and 4 thread blocks
 Read data from the file ../data/zebra-gray-int8-4x 
==PROF== Connected to process 472769 (/pscratch/sd/b/benztimm/csc746hw5/build/sobel_gpu)
 GPU configuration: 4 blocks, 256 threads per block 
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 0: 0%....50%....100% - 13 passes
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 472769
[472769] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-06 00:28:43, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.30
    gpu__time_duration.avg                                                         msecond                          60.88
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           3.70
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.21
    SM Frequency                                                             cycle/usecond                         764.45
    Elapsed Cycles                                                                   cycle                     46,542,934
    Memory [%]                                                                           %                           0.73
    DRAM Throughput                                                                      %                           0.30
    Duration                                                                       msecond                          60.88
    L1/TEX Cache Throughput                                                              %                          19.53
    L2 Cache Throughput                                                                  %                           0.46
    SM Active Cycles                                                                 cycle                   1,730,842.85
    Compute (SM) [%]                                                                     %                           0.78
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full
          waves across all SMs. Look at Launch Statistics for more details.

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           4
    Registers Per Thread                                                   register/thread                             32
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          1,024
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 108
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel
          concurrently with other workloads, consider reducing the block size to have at least one block per
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)
          description for more details on launch configurations.

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              8
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              8
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          12.50
    Achieved Active Warps Per SM                                                      warp                           8.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated
          theoretical (100.0%) and measured achieved occupancy (12.5%) can be the result of warp scheduling overheads
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on
          optimizing occupancy.

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                      5,730,733
    Branch Efficiency                                                                    %                          99.77
    Avg. Divergent Branches                                                                                         17.96
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 6851808 excessive sectors (9% of the
          total 73160558 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source
          locations. The CUDA Programming Guide
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional
          information on reducing uncoalesced device memory accesses.

Running with 256 threads per block and 16 thread blocks
 Read data from the file ../data/zebra-gray-int8-4x 
==PROF== Connected to process 473119 (/pscratch/sd/b/benztimm/csc746hw5/build/sobel_gpu)
 GPU configuration: 16 blocks, 256 threads per block 
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 0: 0%....50%....100% - 13 passes
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 473119
[473119] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-06 00:28:49, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           1.17
    gpu__time_duration.avg                                                         msecond                          15.53
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                          14.82
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.21
    SM Frequency                                                             cycle/usecond                         764.03
    Elapsed Cycles                                                                   cycle                     11,867,430
    Memory [%]                                                                           %                           2.79
    DRAM Throughput                                                                      %                           1.17
    Duration                                                                       msecond                          15.53
    L1/TEX Cache Throughput                                                              %                          18.87
    L2 Cache Throughput                                                                  %                           2.24
    SM Active Cycles                                                                 cycle                   1,751,743.94
    Compute (SM) [%]                                                                     %                           3.06
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full
          waves across all SMs. Look at Launch Statistics for more details.

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                          16
    Registers Per Thread                                                   register/thread                             32
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          4,096
    Waves Per SM                                                                                                     0.02
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 108
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel
          concurrently with other workloads, consider reducing the block size to have at least one block per
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)
          description for more details on launch configurations.

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              8
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              8
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          12.50
    Achieved Active Warps Per SM                                                      warp                           8.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated
          theoretical (100.0%) and measured achieved occupancy (12.5%) can be the result of warp scheduling overheads
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on
          optimizing occupancy.

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                      5,730,925
    Branch Efficiency                                                                    %                          99.77
    Avg. Divergent Branches                                                                                         17.96
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 6851808 excessive sectors (9% of the
          total 73160558 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source
          locations. The CUDA Programming Guide
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional
          information on reducing uncoalesced device memory accesses.

Running with 256 threads per block and 64 thread blocks
 Read data from the file ../data/zebra-gray-int8-4x 
==PROF== Connected to process 473379 (/pscratch/sd/b/benztimm/csc746hw5/build/sobel_gpu)
 GPU configuration: 64 blocks, 256 threads per block 
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 0: 0%....50%....100% - 13 passes
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 473379
[473379] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-06 00:28:52, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           4.34
    gpu__time_duration.avg                                                         msecond                           4.16
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                          58.97
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.21
    SM Frequency                                                             cycle/usecond                         763.83
    Elapsed Cycles                                                                   cycle                      3,181,251
    Memory [%]                                                                           %                          10.21
    DRAM Throughput                                                                      %                           4.34
    Duration                                                                       msecond                           4.16
    L1/TEX Cache Throughput                                                              %                          17.28
    L2 Cache Throughput                                                                  %                           7.37
    SM Active Cycles                                                                 cycle                   1,878,730.96
    Compute (SM) [%]                                                                     %                          11.40
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full
          waves across all SMs. Look at Launch Statistics for more details.

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                          64
    Registers Per Thread                                                   register/thread                             32
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                         16,384
    Waves Per SM                                                                                                     0.07
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 108
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel
          concurrently with other workloads, consider reducing the block size to have at least one block per
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)
          description for more details on launch configurations.

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              8
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              8
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          12.49
    Achieved Active Warps Per SM                                                      warp                           7.99
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated
          theoretical (100.0%) and measured achieved occupancy (12.5%) can be the result of warp scheduling overheads
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on
          optimizing occupancy.

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                      5,731,693
    Branch Efficiency                                                                    %                          99.77
    Avg. Divergent Branches                                                                                         17.96
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 6851808 excessive sectors (9% of the
          total 73160558 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source
          locations. The CUDA Programming Guide
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional
          information on reducing uncoalesced device memory accesses.

Running with 256 threads per block and 256 thread blocks
 Read data from the file ../data/zebra-gray-int8-4x 
==PROF== Connected to process 473459 (/pscratch/sd/b/benztimm/csc746hw5/build/sobel_gpu)
 GPU configuration: 256 blocks, 256 threads per block 
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 0: 0%....50%....100% - 13 passes
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 473459
[473459] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-06 00:28:56, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                          14.18
    gpu__time_duration.avg                                                         msecond                           1.27
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                          96.74
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.22
    SM Frequency                                                             cycle/usecond                         767.53
    Elapsed Cycles                                                                   cycle                        975,106
    Memory [%]                                                                           %                          34.00
    DRAM Throughput                                                                      %                          14.18
    Duration                                                                       msecond                           1.27
    L1/TEX Cache Throughput                                                              %                          35.07
    L2 Cache Throughput                                                                  %                          23.50
    SM Active Cycles                                                                 cycle                     945,415.53
    Compute (SM) [%]                                                                     %                          37.20
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full
          waves across all SMs. Look at Launch Statistics for more details.

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         256
    Registers Per Thread                                                   register/thread                             32
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                         65,536
    Waves Per SM                                                                                                     0.30
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              8
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              8
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          29.14
    Achieved Active Warps Per SM                                                      warp                          18.65
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated
          theoretical (100.0%) and measured achieved occupancy (29.1%) can be the result of warp scheduling overheads
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on
          optimizing occupancy.

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                      5,734,765
    Branch Efficiency                                                                    %                          99.77
    Avg. Divergent Branches                                                                                         17.96
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 6851808 excessive sectors (9% of the
          total 73160558 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source
          locations. The CUDA Programming Guide
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional
          information on reducing uncoalesced device memory accesses.

Running with 256 threads per block and 1024 thread blocks
 Read data from the file ../data/zebra-gray-int8-4x 
==PROF== Connected to process 473506 (/pscratch/sd/b/benztimm/csc746hw5/build/sobel_gpu)
 GPU configuration: 1024 blocks, 256 threads per block 
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 0: 0%....50%....100% - 13 passes
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 473506
[473506] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-06 00:28:59, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                          26.92
    gpu__time_duration.avg                                                         usecond                         775.04
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                          96.66
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.22
    SM Frequency                                                             cycle/usecond                         766.61
    Elapsed Cycles                                                                   cycle                        594,163
    Memory [%]                                                                           %                          56.70
    DRAM Throughput                                                                      %                          26.92
    Duration                                                                       usecond                         775.04
    L1/TEX Cache Throughput                                                              %                          58.73
    L2 Cache Throughput                                                                  %                          40.37
    SM Active Cycles                                                                 cycle                     573,626.78
    Compute (SM) [%]                                                                     %                          61.10
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced.
          Check both the Compute Workload Analysis and Memory Workload Analysis sections.

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                       1,024
    Registers Per Thread                                                   register/thread                             32
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                        262,144
    Waves Per SM                                                                                                     1.19
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 159 thread blocks.
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.4%. Try launching a grid with no
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for
          a grid. See the Hardware Model
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more
          details on launch configurations.

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              8
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              8
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          72.55
    Achieved Active Warps Per SM                                                      warp                          46.43
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated
          theoretical (100.0%) and measured achieved occupancy (72.6%) can be the result of warp scheduling overheads
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on
          optimizing occupancy.

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                      5,747,053
    Branch Efficiency                                                                    %                          99.77
    Avg. Divergent Branches                                                                                         17.96
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 6851808 excessive sectors (9% of the
          total 73160558 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source
          locations. The CUDA Programming Guide
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional
          information on reducing uncoalesced device memory accesses.

Running with 256 threads per block and 4096 thread blocks
 Read data from the file ../data/zebra-gray-int8-4x 
==PROF== Connected to process 473552 (/pscratch/sd/b/benztimm/csc746hw5/build/sobel_gpu)
 GPU configuration: 4096 blocks, 256 threads per block 
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 0: 0%....50%....100% - 13 passes
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 473552
[473552] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-06 00:29:02, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                          28.64
    gpu__time_duration.avg                                                         usecond                         639.17
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                          98.56
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.21
    SM Frequency                                                             cycle/usecond                         763.02
    Elapsed Cycles                                                                   cycle                        487,701
    Memory [%]                                                                           %                          69.03
    DRAM Throughput                                                                      %                          28.64
    Duration                                                                       usecond                         639.17
    L1/TEX Cache Throughput                                                              %                          69.95
    L2 Cache Throughput                                                                  %                          48.82
    SM Active Cycles                                                                 cycle                     481,274.82
    Compute (SM) [%]                                                                     %                          74.64
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced.
          Check both the Compute Workload Analysis and Memory Workload Analysis sections.

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                       4,096
    Registers Per Thread                                                   register/thread                             32
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                      1,048,576
    Waves Per SM                                                                                                     4.74
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              8
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              8
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          91.56
    Achieved Active Warps Per SM                                                      warp                          58.60
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   This kernel's theoretical occupancy is not impacted by any block limit.

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                      5,796,205
    Branch Efficiency                                                                    %                          99.77
    Avg. Divergent Branches                                                                                         17.96
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 6851808 excessive sectors (9% of the
          total 73160558 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source
          locations. The CUDA Programming Guide
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional
          information on reducing uncoalesced device memory accesses.

Running with 512 threads per block and 1 thread blocks
 Read data from the file ../data/zebra-gray-int8-4x 
==PROF== Connected to process 473592 (/pscratch/sd/b/benztimm/csc746hw5/build/sobel_gpu)
 GPU configuration: 1 blocks, 512 threads per block 
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 0: 0%....50%....100% - 13 passes
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 473592
[473592] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-06 00:29:27, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.13
    gpu__time_duration.avg                                                         msecond                         139.39
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           0.93
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.22
    SM Frequency                                                             cycle/usecond                         766.07
    Elapsed Cycles                                                                   cycle                    106,780,551
    Memory [%]                                                                           %                           0.33
    DRAM Throughput                                                                      %                           0.13
    Duration                                                                       msecond                         139.39
    L1/TEX Cache Throughput                                                              %                          35.83
    L2 Cache Throughput                                                                  %                           0.18
    SM Active Cycles                                                                 cycle                     975,497.95
    Compute (SM) [%]                                                                     %                           0.34
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full
          waves across all SMs. Look at Launch Statistics for more details.

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        512
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             32
    Shared Memory Configuration Size                                                 Kbyte                          16.38
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                            512
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel
          concurrently with other workloads, consider reducing the block size to have at least one block per
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)
          description for more details on launch configurations.

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              4
    Block Limit Shared Mem                                                           block                             16
    Block Limit Warps                                                                block                              4
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          24.99
    Achieved Active Warps Per SM                                                      warp                          15.99
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated
          theoretical (100.0%) and measured achieved occupancy (25.0%) can be the result of warp scheduling overheads
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on
          optimizing occupancy.

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                      5,730,701
    Branch Efficiency                                                                    %                          99.77
    Avg. Divergent Branches                                                                                         17.96
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 6851808 excessive sectors (9% of the
          total 73160558 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source
          locations. The CUDA Programming Guide
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional
          information on reducing uncoalesced device memory accesses.

Running with 512 threads per block and 4 thread blocks
 Read data from the file ../data/zebra-gray-int8-4x 
==PROF== Connected to process 475175 (/pscratch/sd/b/benztimm/csc746hw5/build/sobel_gpu)
 GPU configuration: 4 blocks, 512 threads per block 
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 0: 0%....50%....100% - 13 passes
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 475175
[475175] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-06 00:29:35, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.52
    gpu__time_duration.avg                                                         msecond                          34.39
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           3.70
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.22
    SM Frequency                                                             cycle/usecond                         765.83
    Elapsed Cycles                                                                   cycle                     26,338,166
    Memory [%]                                                                           %                           1.31
    DRAM Throughput                                                                      %                           0.52
    Duration                                                                       msecond                          34.39
    L1/TEX Cache Throughput                                                              %                          35.28
    L2 Cache Throughput                                                                  %                           0.95
    SM Active Cycles                                                                 cycle                     975,341.16
    Compute (SM) [%]                                                                     %                           1.38
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full
          waves across all SMs. Look at Launch Statistics for more details.

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        512
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           4
    Registers Per Thread                                                   register/thread                             32
    Shared Memory Configuration Size                                                 Kbyte                          16.38
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          2,048
    Waves Per SM                                                                                                     0.01
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 108
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel
          concurrently with other workloads, consider reducing the block size to have at least one block per
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)
          description for more details on launch configurations.

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              4
    Block Limit Shared Mem                                                           block                             16
    Block Limit Warps                                                                block                              4
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          24.98
    Achieved Active Warps Per SM                                                      warp                          15.99
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated
          theoretical (100.0%) and measured achieved occupancy (25.0%) can be the result of warp scheduling overheads
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on
          optimizing occupancy.

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                      5,730,797
    Branch Efficiency                                                                    %                          99.77
    Avg. Divergent Branches                                                                                         17.96
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 6851808 excessive sectors (9% of the
          total 73160558 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source
          locations. The CUDA Programming Guide
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional
          information on reducing uncoalesced device memory accesses.

Running with 512 threads per block and 16 thread blocks
 Read data from the file ../data/zebra-gray-int8-4x 
==PROF== Connected to process 475422 (/pscratch/sd/b/benztimm/csc746hw5/build/sobel_gpu)
 GPU configuration: 16 blocks, 512 threads per block 
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 0: 0%....50%....100% - 13 passes
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 475422
[475422] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-06 00:29:40, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           1.96
    gpu__time_duration.avg                                                         msecond                           9.24
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                          14.71
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.22
    SM Frequency                                                             cycle/usecond                         765.61
    Elapsed Cycles                                                                   cycle                      7,072,670
    Memory [%]                                                                           %                           4.74
    DRAM Throughput                                                                      %                           1.96
    Duration                                                                       msecond                           9.24
    L1/TEX Cache Throughput                                                              %                          32.20
    L2 Cache Throughput                                                                  %                           3.83
    SM Active Cycles                                                                 cycle                   1,041,589.51
    Compute (SM) [%]                                                                     %                           5.13
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full
          waves across all SMs. Look at Launch Statistics for more details.

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        512
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                          16
    Registers Per Thread                                                   register/thread                             32
    Shared Memory Configuration Size                                                 Kbyte                          16.38
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          8,192
    Waves Per SM                                                                                                     0.04
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 108
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel
          concurrently with other workloads, consider reducing the block size to have at least one block per
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)
          description for more details on launch configurations.

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              4
    Block Limit Shared Mem                                                           block                             16
    Block Limit Warps                                                                block                              4
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          24.76
    Achieved Active Warps Per SM                                                      warp                          15.85
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated
          theoretical (100.0%) and measured achieved occupancy (24.8%) can be the result of warp scheduling overheads
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on
          optimizing occupancy.

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                      5,731,181
    Branch Efficiency                                                                    %                          99.77
    Avg. Divergent Branches                                                                                         17.96
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 6851808 excessive sectors (9% of the
          total 73160558 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source
          locations. The CUDA Programming Guide
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional
          information on reducing uncoalesced device memory accesses.

Running with 512 threads per block and 64 thread blocks
 Read data from the file ../data/zebra-gray-int8-4x 
==PROF== Connected to process 475518 (/pscratch/sd/b/benztimm/csc746hw5/build/sobel_gpu)
 GPU configuration: 64 blocks, 512 threads per block 
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 0: 0%....50%....100% - 13 passes
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 475518
[475518] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-06 00:29:43, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           7.57
    gpu__time_duration.avg                                                         msecond                           2.38
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                          58.73
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.21
    SM Frequency                                                             cycle/usecond                         764.17
    Elapsed Cycles                                                                   cycle                      1,821,368
    Memory [%]                                                                           %                          18.45
    DRAM Throughput                                                                      %                           7.57
    Duration                                                                       msecond                           2.38
    L1/TEX Cache Throughput                                                              %                          31.36
    L2 Cache Throughput                                                                  %                          12.08
    SM Active Cycles                                                                 cycle                   1,071,338.56
    Compute (SM) [%]                                                                     %                          19.91
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full
          waves across all SMs. Look at Launch Statistics for more details.

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        512
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                          64
    Registers Per Thread                                                   register/thread                             32
    Shared Memory Configuration Size                                                 Kbyte                          16.38
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                         32,768
    Waves Per SM                                                                                                     0.15
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 108
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel
          concurrently with other workloads, consider reducing the block size to have at least one block per
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)
          description for more details on launch configurations.

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              4
    Block Limit Shared Mem                                                           block                             16
    Block Limit Warps                                                                block                              4
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          24.73
    Achieved Active Warps Per SM                                                      warp                          15.83
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated
          theoretical (100.0%) and measured achieved occupancy (24.7%) can be the result of warp scheduling overheads
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on
          optimizing occupancy.

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                      5,732,717
    Branch Efficiency                                                                    %                          99.77
    Avg. Divergent Branches                                                                                         17.96
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 6851808 excessive sectors (9% of the
          total 73160558 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source
          locations. The CUDA Programming Guide
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional
          information on reducing uncoalesced device memory accesses.

Running with 512 threads per block and 256 thread blocks
 Read data from the file ../data/zebra-gray-int8-4x 
==PROF== Connected to process 475565 (/pscratch/sd/b/benztimm/csc746hw5/build/sobel_gpu)
 GPU configuration: 256 blocks, 512 threads per block 
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 0: 0%....50%....100% - 13 passes
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 475565
[475565] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-06 00:29:46, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                          21.44
    gpu__time_duration.avg                                                         usecond                         926.56
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                          89.61
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.21
    SM Frequency                                                             cycle/usecond                         761.02
    Elapsed Cycles                                                                   cycle                        705,145
    Memory [%]                                                                           %                          48.02
    DRAM Throughput                                                                      %                          21.44
    Duration                                                                       usecond                         926.56
    L1/TEX Cache Throughput                                                              %                          53.39
    L2 Cache Throughput                                                                  %                          30.59
    SM Active Cycles                                                                 cycle                     634,180.66
    Compute (SM) [%]                                                                     %                          51.46
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.6 full
          waves across all SMs. Look at Launch Statistics for more details.

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        512
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         256
    Registers Per Thread                                                   register/thread                             32
    Shared Memory Configuration Size                                                 Kbyte                          16.38
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                        131,072
    Waves Per SM                                                                                                     0.59
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              4
    Block Limit Shared Mem                                                           block                             16
    Block Limit Warps                                                                block                              4
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          55.37
    Achieved Active Warps Per SM                                                      warp                          35.44
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated
          theoretical (100.0%) and measured achieved occupancy (55.4%) can be the result of warp scheduling overheads
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on
          optimizing occupancy.

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                      5,738,861
    Branch Efficiency                                                                    %                          99.77
    Avg. Divergent Branches                                                                                         17.96
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 6851808 excessive sectors (9% of the
          total 73160558 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source
          locations. The CUDA Programming Guide
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional
          information on reducing uncoalesced device memory accesses.

Running with 512 threads per block and 1024 thread blocks
 Read data from the file ../data/zebra-gray-int8-4x 
==PROF== Connected to process 475620 (/pscratch/sd/b/benztimm/csc746hw5/build/sobel_gpu)
 GPU configuration: 1024 blocks, 512 threads per block 
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 0: 0%....50%....100% - 13 passes
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 475620
[475620] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-06 00:29:50, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                          28.02
    gpu__time_duration.avg                                                         usecond                         701.02
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                          95.41
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.21
    SM Frequency                                                             cycle/usecond                         761.52
    Elapsed Cycles                                                                   cycle                        533,852
    Memory [%]                                                                           %                          63.63
    DRAM Throughput                                                                      %                          28.02
    Duration                                                                       usecond                         701.02
    L1/TEX Cache Throughput                                                              %                          66.57
    L2 Cache Throughput                                                                  %                          43.54
    SM Active Cycles                                                                 cycle                     510,282.11
    Compute (SM) [%]                                                                     %                          68.07
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced.
          Check both the Compute Workload Analysis and Memory Workload Analysis sections.

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        512
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                       1,024
    Registers Per Thread                                                   register/thread                             32
    Shared Memory Configuration Size                                                 Kbyte                          16.38
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                        524,288
    Waves Per SM                                                                                                     2.37
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              4
    Block Limit Shared Mem                                                           block                             16
    Block Limit Warps                                                                block                              4
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          85.55
    Achieved Active Warps Per SM                                                      warp                          54.75
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated
          theoretical (100.0%) and measured achieved occupancy (85.5%) can be the result of warp scheduling overheads
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on
          optimizing occupancy.

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                      5,763,437
    Branch Efficiency                                                                    %                          99.77
    Avg. Divergent Branches                                                                                         17.96
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 6851808 excessive sectors (9% of the
          total 73160558 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source
          locations. The CUDA Programming Guide
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional
          information on reducing uncoalesced device memory accesses.

Running with 512 threads per block and 4096 thread blocks
 Read data from the file ../data/zebra-gray-int8-4x 
==PROF== Connected to process 475834 (/pscratch/sd/b/benztimm/csc746hw5/build/sobel_gpu)
 GPU configuration: 4096 blocks, 512 threads per block 
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 0: 0%....50%....100% - 13 passes
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 475834
[475834] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-06 00:29:53, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                          28.26
    gpu__time_duration.avg                                                         usecond                         644.19
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                          98.33
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.21
    SM Frequency                                                             cycle/usecond                         763.68
    Elapsed Cycles                                                                   cycle                        491,960
    Memory [%]                                                                           %                          68.66
    DRAM Throughput                                                                      %                          28.26
    Duration                                                                       usecond                         644.19
    L1/TEX Cache Throughput                                                              %                          69.78
    L2 Cache Throughput                                                                  %                          47.75
    SM Active Cycles                                                                 cycle                     484,062.88
    Compute (SM) [%]                                                                     %                          74.25
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced.
          Check both the Compute Workload Analysis and Memory Workload Analysis sections.

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        512
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                       4,096
    Registers Per Thread                                                   register/thread                             32
    Shared Memory Configuration Size                                                 Kbyte                          16.38
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                      2,097,152
    Waves Per SM                                                                                                     9.48
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              4
    Block Limit Shared Mem                                                           block                             16
    Block Limit Warps                                                                block                              4
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          92.78
    Achieved Active Warps Per SM                                                      warp                          59.38
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   This kernel's theoretical occupancy is not impacted by any block limit.

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                      5,861,741
    Branch Efficiency                                                                    %                          99.77
    Avg. Divergent Branches                                                                                         17.96
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 6851808 excessive sectors (9% of the
          total 73160558 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source
          locations. The CUDA Programming Guide
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional
          information on reducing uncoalesced device memory accesses.

Running with 1024 threads per block and 1 thread blocks
 Read data from the file ../data/zebra-gray-int8-4x 
==PROF== Connected to process 475927 (/pscratch/sd/b/benztimm/csc746hw5/build/sobel_gpu)
 GPU configuration: 1 blocks, 1024 threads per block 
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 0: 0%....50%....100% - 13 passes
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 475927
[475927] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-06 00:30:11, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.23
    gpu__time_duration.avg                                                         msecond                          79.31
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           0.92
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.22
    SM Frequency                                                             cycle/usecond                         766.75
    Elapsed Cycles                                                                   cycle                     60,810,781
    Memory [%]                                                                           %                           0.57
    DRAM Throughput                                                                      %                           0.23
    Duration                                                                       msecond                          79.31
    L1/TEX Cache Throughput                                                              %                          62.05
    L2 Cache Throughput                                                                  %                           0.32
    SM Active Cycles                                                                 cycle                     562,525.92
    Compute (SM) [%]                                                                     %                           0.60
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full
          waves across all SMs. Look at Launch Statistics for more details.

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                      1,024
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             32
    Shared Memory Configuration Size                                                 Kbyte                           8.19
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          1,024
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel
          concurrently with other workloads, consider reducing the block size to have at least one block per
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)
          description for more details on launch configurations.

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              2
    Block Limit Shared Mem                                                           block                              8
    Block Limit Warps                                                                block                              2
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          49.93
    Achieved Active Warps Per SM                                                      warp                          31.95
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated
          theoretical (100.0%) and measured achieved occupancy (49.9%) can be the result of warp scheduling overheads
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on
          optimizing occupancy.

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                      5,730,733
    Branch Efficiency                                                                    %                          99.77
    Avg. Divergent Branches                                                                                         17.96
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 6851808 excessive sectors (9% of the
          total 73160558 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source
          locations. The CUDA Programming Guide
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional
          information on reducing uncoalesced device memory accesses.

Running with 1024 threads per block and 4 thread blocks
 Read data from the file ../data/zebra-gray-int8-4x 
==PROF== Connected to process 476622 (/pscratch/sd/b/benztimm/csc746hw5/build/sobel_gpu)
 GPU configuration: 4 blocks, 1024 threads per block 
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 0: 0%....50%....100% - 13 passes
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 476622
[476622] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-06 00:30:17, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.85
    gpu__time_duration.avg                                                         msecond                          21.32
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           3.70
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.21
    SM Frequency                                                             cycle/usecond                         764.36
    Elapsed Cycles                                                                   cycle                     16,294,180
    Memory [%]                                                                           %                           2.03
    DRAM Throughput                                                                      %                           0.85
    Duration                                                                       msecond                          21.32
    L1/TEX Cache Throughput                                                              %                          54.81
    L2 Cache Throughput                                                                  %                           1.59
    SM Active Cycles                                                                 cycle                     603,039.96
    Compute (SM) [%]                                                                     %                           2.23
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full
          waves across all SMs. Look at Launch Statistics for more details.

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                      1,024
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           4
    Registers Per Thread                                                   register/thread                             32
    Shared Memory Configuration Size                                                 Kbyte                           8.19
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          4,096
    Waves Per SM                                                                                                     0.02
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 108
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel
          concurrently with other workloads, consider reducing the block size to have at least one block per
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)
          description for more details on launch configurations.

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              2
    Block Limit Shared Mem                                                           block                              8
    Block Limit Warps                                                                block                              2
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          48.97
    Achieved Active Warps Per SM                                                      warp                          31.34
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated
          theoretical (100.0%) and measured achieved occupancy (49.0%) can be the result of warp scheduling overheads
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on
          optimizing occupancy.

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                      5,730,925
    Branch Efficiency                                                                    %                          99.77
    Avg. Divergent Branches                                                                                         17.96
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 6851808 excessive sectors (9% of the
          total 73160558 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source
          locations. The CUDA Programming Guide
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional
          information on reducing uncoalesced device memory accesses.

Running with 1024 threads per block and 16 thread blocks
 Read data from the file ../data/zebra-gray-int8-4x 
==PROF== Connected to process 476701 (/pscratch/sd/b/benztimm/csc746hw5/build/sobel_gpu)
 GPU configuration: 16 blocks, 1024 threads per block 
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 0: 0%....50%....100% - 13 passes
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 476701
[476701] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-06 00:30:22, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           3.31
    gpu__time_duration.avg                                                         msecond                           5.46
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                          14.72
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.21
    SM Frequency                                                             cycle/usecond                         764.28
    Elapsed Cycles                                                                   cycle                      4,170,487
    Memory [%]                                                                           %                           8.11
    DRAM Throughput                                                                      %                           3.31
    Duration                                                                       msecond                           5.46
    L1/TEX Cache Throughput                                                              %                          54.96
    L2 Cache Throughput                                                                  %                           5.61
    SM Active Cycles                                                                 cycle                     615,290.27
    Compute (SM) [%]                                                                     %                           8.70
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full
          waves across all SMs. Look at Launch Statistics for more details.

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                      1,024
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                          16
    Registers Per Thread                                                   register/thread                             32
    Shared Memory Configuration Size                                                 Kbyte                           8.19
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                         16,384
    Waves Per SM                                                                                                     0.07
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 108
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel
          concurrently with other workloads, consider reducing the block size to have at least one block per
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)
          description for more details on launch configurations.

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              2
    Block Limit Shared Mem                                                           block                              8
    Block Limit Warps                                                                block                              2
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          48.83
    Achieved Active Warps Per SM                                                      warp                          31.25
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated
          theoretical (100.0%) and measured achieved occupancy (48.8%) can be the result of warp scheduling overheads
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on
          optimizing occupancy.

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                      5,731,693
    Branch Efficiency                                                                    %                          99.77
    Avg. Divergent Branches                                                                                         17.96
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 6851808 excessive sectors (9% of the
          total 73160558 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source
          locations. The CUDA Programming Guide
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional
          information on reducing uncoalesced device memory accesses.

Running with 1024 threads per block and 64 thread blocks
 Read data from the file ../data/zebra-gray-int8-4x 
==PROF== Connected to process 476970 (/pscratch/sd/b/benztimm/csc746hw5/build/sobel_gpu)
 GPU configuration: 64 blocks, 1024 threads per block 
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 0: 0%....50%....100% - 13 passes
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 476970
[476970] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-06 00:30:25, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                          12.28
    gpu__time_duration.avg                                                         msecond                           1.47
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                          58.30
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.22
    SM Frequency                                                             cycle/usecond                         766.32
    Elapsed Cycles                                                                   cycle                      1,123,082
    Memory [%]                                                                           %                          30.24
    DRAM Throughput                                                                      %                          12.28
    Duration                                                                       msecond                           1.47
    L1/TEX Cache Throughput                                                              %                          51.76
    L2 Cache Throughput                                                                  %                          18.46
    SM Active Cycles                                                                 cycle                     656,150.11
    Compute (SM) [%]                                                                     %                          32.30
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full
          waves across all SMs. Look at Launch Statistics for more details.

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                      1,024
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                          64
    Registers Per Thread                                                   register/thread                             32
    Shared Memory Configuration Size                                                 Kbyte                           8.19
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                         65,536
    Waves Per SM                                                                                                     0.30
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 108
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel
          concurrently with other workloads, consider reducing the block size to have at least one block per
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)
          description for more details on launch configurations.

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              2
    Block Limit Shared Mem                                                           block                              8
    Block Limit Warps                                                                block                              2
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          48.60
    Achieved Active Warps Per SM                                                      warp                          31.10
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated
          theoretical (100.0%) and measured achieved occupancy (48.6%) can be the result of warp scheduling overheads
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on
          optimizing occupancy.

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                      5,734,765
    Branch Efficiency                                                                    %                          99.77
    Avg. Divergent Branches                                                                                         17.96
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 6851808 excessive sectors (9% of the
          total 73160558 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source
          locations. The CUDA Programming Guide
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional
          information on reducing uncoalesced device memory accesses.

Running with 1024 threads per block and 256 thread blocks
 Read data from the file ../data/zebra-gray-int8-4x 
==PROF== Connected to process 477024 (/pscratch/sd/b/benztimm/csc746hw5/build/sobel_gpu)
 GPU configuration: 256 blocks, 1024 threads per block 
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 0: 0%....50%....100% - 13 passes
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 477024
[477024] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-06 00:30:28, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                          23.71
    gpu__time_duration.avg                                                         usecond                         879.01
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                          77.41
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.22
    SM Frequency                                                             cycle/usecond                         766.74
    Elapsed Cycles                                                                   cycle                        673,985
    Memory [%]                                                                           %                          50.60
    DRAM Throughput                                                                      %                          23.71
    Duration                                                                       usecond                         879.01
    L1/TEX Cache Throughput                                                              %                          65.12
    L2 Cache Throughput                                                                  %                          32.76
    SM Active Cycles                                                                 cycle                     523,683.51
    Compute (SM) [%]                                                                     %                          53.87
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                      1,024
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         256
    Registers Per Thread                                                   register/thread                             32
    Shared Memory Configuration Size                                                 Kbyte                           8.19
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                        262,144
    Waves Per SM                                                                                                     1.19
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              2
    Block Limit Shared Mem                                                           block                              8
    Block Limit Warps                                                                block                              2
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          83.94
    Achieved Active Warps Per SM                                                      warp                          53.72
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated
          theoretical (100.0%) and measured achieved occupancy (83.9%) can be the result of warp scheduling overheads
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on
          optimizing occupancy.

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                      5,747,053
    Branch Efficiency                                                                    %                          99.77
    Avg. Divergent Branches                                                                                         17.96
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 6851808 excessive sectors (9% of the
          total 73160558 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source
          locations. The CUDA Programming Guide
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional
          information on reducing uncoalesced device memory accesses.

Running with 1024 threads per block and 1024 thread blocks
 Read data from the file ../data/zebra-gray-int8-4x 
==PROF== Connected to process 477243 (/pscratch/sd/b/benztimm/csc746hw5/build/sobel_gpu)
 GPU configuration: 1024 blocks, 1024 threads per block 
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 0: 0%....50%....100% - 13 passes
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 477243
[477243] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-06 00:30:32, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                          28.04
    gpu__time_duration.avg                                                         usecond                         690.91
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                          94.36
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.21
    SM Frequency                                                             cycle/usecond                         764.95
    Elapsed Cycles                                                                   cycle                        528,525
    Memory [%]                                                                           %                          64.24
    DRAM Throughput                                                                      %                          28.04
    Duration                                                                       usecond                         690.91
    L1/TEX Cache Throughput                                                              %                          67.96
    L2 Cache Throughput                                                                  %                          47.19
    SM Active Cycles                                                                 cycle                     499,576.31
    Compute (SM) [%]                                                                     %                          68.88
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced.
          Check both the Compute Workload Analysis and Memory Workload Analysis sections.

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                      1,024
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                       1,024
    Registers Per Thread                                                   register/thread                             32
    Shared Memory Configuration Size                                                 Kbyte                           8.19
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                      1,048,576
    Waves Per SM                                                                                                     4.74
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              2
    Block Limit Shared Mem                                                           block                              8
    Block Limit Warps                                                                block                              2
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          91.19
    Achieved Active Warps Per SM                                                      warp                          58.36
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   This kernel's theoretical occupancy is not impacted by any block limit.

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                      5,796,205
    Branch Efficiency                                                                    %                          99.77
    Avg. Divergent Branches                                                                                         17.96
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 6851808 excessive sectors (9% of the
          total 73160558 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source
          locations. The CUDA Programming Guide
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional
          information on reducing uncoalesced device memory accesses.

Running with 1024 threads per block and 4096 thread blocks
 Read data from the file ../data/zebra-gray-int8-4x 
==PROF== Connected to process 477276 (/pscratch/sd/b/benztimm/csc746hw5/build/sobel_gpu)
 GPU configuration: 4096 blocks, 1024 threads per block 
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 0: 0%....50%....100% - 13 passes
 Wrote the output file ../data/processed-raw-int8-4x-cpu.dat 
==PROF== Disconnected from process 477276
[477276] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-06 00:30:35, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                          26.70
    gpu__time_duration.avg                                                         usecond                         675.90
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                          98.19
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.21
    SM Frequency                                                             cycle/usecond                         764.51
    Elapsed Cycles                                                                   cycle                        516,742
    Memory [%]                                                                           %                          65.28
    DRAM Throughput                                                                      %                          26.70
    Duration                                                                       usecond                         675.90
    L1/TEX Cache Throughput                                                              %                          66.43
    L2 Cache Throughput                                                                  %                          46.50
    SM Active Cycles                                                                 cycle                     507,735.24
    Compute (SM) [%]                                                                     %                          71.19
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced.
          Check both the Compute Workload Analysis and Memory Workload Analysis sections.

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                      1,024
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                       4,096
    Registers Per Thread                                                   register/thread                             32
    Shared Memory Configuration Size                                                 Kbyte                           8.19
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                      4,194,304
    Waves Per SM                                                                                                    18.96
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              2
    Block Limit Shared Mem                                                           block                              8
    Block Limit Warps                                                                block                              2
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          91.43
    Achieved Active Warps Per SM                                                      warp                          58.51
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   This kernel's theoretical occupancy is not impacted by any block limit.

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                      5,992,813
    Branch Efficiency                                                                    %                          99.77
    Avg. Divergent Branches                                                                                         17.96
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 6851808 excessive sectors (9% of the
          total 73160558 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source
          locations. The CUDA Programming Guide
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional
          information on reducing uncoalesced device memory accesses.

benztimm@perlmutter:login26:/pscratch/sd/b/benztimm/csc746hw5/build> 